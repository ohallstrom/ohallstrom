# 👋 Hello! 
### I'm Oskar, Machine Learning Engineer within R&D of Large Language Models
- 👨🏼‍💻 Currently aligning Large Language Models @ [LightOn](https://github.com/lightonai)
  - Conducted [experiments with WSD-scheduler and novel positional weighting in loss on the Mamba Architecture](https://www.lighton.ai/fr/blog/blog-4/passing-the-torch-training-a-mamba-model-for-smooth-handover-54) 
  - Project lead in the creation of [Alfred-40B-1023](https://huggingface.co/lightonai/alfred-40b-1023), using a custom extension method to obtain an 8K context length
  - Responsible for reward modeling in the creation of [Alfred-40B-0723](https://huggingface.co/lightonai/alfred-40b-0723) - a RLHF:ed version of Falcon-40B. Alfred was the second ever open source LLM aligned with PPO (Llama 2 being the first one).   
- 🗒 MSc in Computer Science specialized in Machine Learning  
  - [Master's Thesis in Synthetic Data Generation for Large Language Model Reward Modeling](https://github.com/ohallstrom/ohallstrom/blob/main/modeling_of_human_preferences_without_humans.pdf)
  - Included 65 credits at École Polytechnique Fédérale de Lausanne
- 👀 Interested in everything generative AI
- 📫 You can reach me on [LinkedIn](https://www.linkedin.com/in/oskar-hallström-b747a7114/)
- 🎸 Check out my Indie Band [Billie Garlic](https://open.spotify.com/artist/2KZoVTprHSLoYX7G38MBh9?si=2-ojbn-iS7-_sTpeDI-dTw)

<!---
ohallstrom/ohallstrom is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
