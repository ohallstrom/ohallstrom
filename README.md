# 👋 Hello! 
### I'm Oskar, Machine Learning Engineer within R&D of Large Language Models
- 👨🏼‍💻 Currently aligning Large Language Models @ [LightOn](https://github.com/lightonai)
  - Debugged very odd bugs, such as when I found [torch.randperm to be not actually random](https://x.com/oskar_hallstrom/status/1869792390934082029)
  - Contributed to the model arch and training of the SOTA encoder [ModernBERT](https://huggingface.co/blog/modernbert)
  - Did a small side mission experimenting with [a novel positional weighting in pretraining](https://www.lighton.ai/lighton-blogs/passing-the-torch-training-a-mamba-model-for-smooth-handover) 
  - Project lead in the creation of [Alfred-40B-1023](https://huggingface.co/lightonai/alfred-40b-1023), using a custom extension method to obtain an extended context length
  - Responsible for reward modeling in the creation of [Alfred-40B-0723](https://huggingface.co/lightonai/alfred-40b-0723) - a RLHF:ed version of Falcon-40B. Alfred was one of the first ever open LLMs aligned with PPO.
- 🗒 MSc in Computer Science specialized in Machine Learning  
  - [Master's Thesis in Synthetic Data Generation for Large Language Model Reward Modeling](https://github.com/ohallstrom/ohallstrom/blob/main/modeling_of_human_preferences_without_humans.pdf)
  - Included 65 credits at École Polytechnique Fédérale de Lausanne
- 👀 Interested in everything generative AI
- 📫 You can reach me on [LinkedIn](https://www.linkedin.com/in/oskar-hallström-b747a7114/) or [X/Twitter](https://x.com/oskar_hallstrom/)
- 🎸 Check out my Indie Band [Billie Garlic](https://open.spotify.com/artist/2KZoVTprHSLoYX7G38MBh9?si=2-ojbn-iS7-_sTpeDI-dTw)

<!---
ohallstrom/ohallstrom is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
